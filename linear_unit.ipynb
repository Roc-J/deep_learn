{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 往期回顾\n",
    "\n",
    "在上一篇文章中，已经学会了编写一个简单的感知器，并用它来实现一个线性分类器。你应该还记得用来训练感知器的【感知器规则】。然而，我们并不关心这个规则是怎么得到的。本文通过介绍另外一种感知器，也就是【线性单元】，来说明关于机器学习中一些基本的概念，比如模型，目标函数，优化算法等。这些概念对于所有的机器学习算法来说是通用的，掌握了这些概念，就掌握了机器学习的基本套路。\n",
    "\n",
    "## 线性单元是啥\n",
    "感知器有一个问题，当面对的数据集不是线性可分的时候，【感知器规则】可能无法收敛，这意味着我们永远也无法完成一个感知器的训练，为了解决这个问题，我们使用一个可导的线性函数来替代感知器的阶跃函数，这种感知器叫就做线性单元。线性单元面对线性不可分的数据集时，会收敛到一个最佳的近似上。\n",
    "\n",
    "为了简单起见，我们可以设置线性单元的激活函数f \n",
    "\n",
    "线性单元如下图所示：\n",
    "![](http://upload-images.jianshu.io/upload_images/2256672-f57602e423d739ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "对比我们的感知器： \n",
    "![](http://upload-images.jianshu.io/upload_images/2256672-801d65e79bfc3162.png)\n",
    "\n",
    "这样替换了激活函数后，线性单元将返回一个实数值而不是0,1分类。因此线性单元用来解决回归问题而不是分类问题。\n",
    "\n",
    "## 线性单元的模型\n",
    "\n",
    "当我们说模型时，实际上在谈论根据输入x预测输出y的算法。比如，x可以是一个人的工作年限，y可以是它的月薪，我们可以用某种算法来根据一个人的工作年限来预测它的收入。\n",
    "\n",
    "y = h(x) = w * x + b\n",
    " \n",
    "函数h(x)叫做假设，而w,b是它的参数。我们假设参数w = 1000, 参数b = 500, 如果一个人的工作年限是5年的话，我们的模型会预测年薪为\n",
    "\n",
    "y = h(x) = 1000 * 5 + 500 = 5500\n",
    "\n",
    "你也许会说，这个模型太不靠谱。是这样的，因为我们考虑的因素太少了，仅仅包含了工作年限。如果考虑更多的因素，比如所处的行业，公司，职级等等，可能预测的就会靠谱的多。我们把工作年限，行业、公司、职级这些信息，称之为特征。对于一个工作了5年，在IT行业，百度工作，职级T6这样的人，我们可以用这样的一个特征向量来表示他 X = (5， IT, 百度， T6）\n",
    "既然输入x变成了一个具备四个特征的向量，相对应的，仅仅一个参数w就不够用了，我们应该使用4个参数w1, w2, w3, w4，每个特征对应一个\n",
    "\n",
    "y = h(x) = w1 * x1 + w2 * x2 + w3 * x3 +w4 * x4 + b\n",
    "\n",
    "其中，x1对应工作年限，x2对应行业，x3对应公司，x4对应职级。\n",
    "\n",
    "为了书写和计算方便，我们可以令w0等于b，同时令w0对应于特征x0。由于x0其实并不存在，我们可以令它的值永远为1。也就是说\n",
    "\n",
    "b = w0 * x0 ，其中x0=1\n",
    "\n",
    "这样上面的式子就可以写为y= h(x）= w1 * x1 + w2 * x2 + w3 * x3 + w4 * x4 + b\n",
    "\n",
    "y= h(x）= w0 * x0 +  w1 * x1 + w2 * x2 + w3 * x3 + w4 * x4\n",
    "\n",
    "或用向量表示\n",
    "\n",
    "长成这样线性模型。\n",
    "\n",
    "## 线性单元的目标函数\n",
    "\n",
    "## 梯度下降优化算法\n",
    "\n",
    "![](http://upload-images.jianshu.io/upload_images/2256672-46acc2c2d52fc366.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/480)\n",
    "\n",
    "## 随机梯度下降算法(Stochastic Gradient Descent, SGD)\n",
    "\n",
    "如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。因此，实用的算法是SGD算法。在SGD算法中，每次更新w的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对w更新数百万次，效率大大提升。由于样本的噪音和随机性，每次更新w并不一定按照减少E的方向。然而，虽然存在一定随机性，大量的更新总体上沿着E减少的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别\n",
    "\n",
    "![](http://upload-images.jianshu.io/upload_images/2256672-3152002d503d768e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。\n",
    "\n",
    "最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perception import Perception\n",
    "\n",
    "# 定义激活函数\n",
    "f = lambda x: x\n",
    "\n",
    "class LinearUnit(Perception):\n",
    "    def __init__(self, input_num):\n",
    "        '''\n",
    "        初始化线性单元，设置输入参数的个数\n",
    "        '''\n",
    "        Perception.__init__(self, input_num, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset():\n",
    "    '''\n",
    "    捏造5个人的收入数据\n",
    "    \n",
    "    '''\n",
    "    input_vecs = [\n",
    "        [5],[3],[8],[1.4],[10.1]\n",
    "    ]\n",
    "    labels = [5500, 2300, 7600, 1800, 11400]\n",
    "\n",
    "    return input_vecs, labels\n",
    "\n",
    "def train_linear_unit():\n",
    "    lu = LinearUnit(1)\n",
    "    input_vecs, labels = get_training_dataset()\n",
    "    lu.train(input_vecs, labels, 10, 0.01)\n",
    "    return lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights\t: [1124.0634970262222]\n",
      "bias:\t: 85.485289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    linear_unit = train_linear_unit()\n",
    "    print linear_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Work 3.4 years, monthly salary = 3907.30\n",
      "Work 15 years, monthly salary = 16946.44\n",
      "Work 1.5 years, monthly salary = 1771.58\n",
      "Work 6.3 years, monthly salary = 7167.09\n"
     ]
    }
   ],
   "source": [
    "print 'Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4])\n",
    "print 'Work 15 years, monthly salary = %.2f' % linear_unit.predict([15])\n",
    "print 'Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5])\n",
    "print 'Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "事实上，一个机器学习算法其实只有两部分\n",
    "* 模型 从输入特征x预测输出y的那个函数\n",
    "* 目标函数 目标函数取最小（最大）值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小（最大）值，因此也只能得到模型参数的局部最优值。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
